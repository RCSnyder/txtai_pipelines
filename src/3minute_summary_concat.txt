This week we focus on some of the organizational things that you need to do in order to work together on ML powered products as part of an interdisciplinary team. Machine learning talent tends to be very scarce and expensive to attract. Machinelearning projects often have unclear timelines, and there's a high degree of uncertainty to those timelines.
The goal is to help the rest of the stakeholders in your organization understand what they can really expect from the technology that you're building and what is realistic for us to achieve. We're going to talk about some of the unique aspects involved in hiring. or ML platform or ML info teams, machine learning engineers, machinelearning researchers, or ML scientists, data scientists.
ML engineer is kind of a catch all is the ML researcher. This is a role that exists in some organizations that the responsibility stops after the model has been trained. These folks are focused on building models that are not yet production critical or forward looking. They're working with tools like AWS, like Kafka or other data infrastructure tools and potentially working with ML infrastructure vendors.
Data science is a distinct function that is responsible for answering business questions using data. The ML work is the responsibility of an ML team. ML ops or ML platform engineers is they source them from MLEs at their organization. This tends to be a rare mix of ML skills and software engineering skills.
Data science is kind of like a catch all term for a bunch of different roles in different organizations, it also admits a variety of different backgrounds. Some ML engineers are really responsible for like one ML pipeline or maybe a handful of ML pipelines. And then lastly, MLPMs, oftentimes these folks come from a traditional product management background.
How to think about hiring ML specialists. If you are an ML specialist looking for a job, how to make yourself more attractive as a job candidate. How to source for ML engineers if you're hiring folks. We'll talk about interviewing and then last say we will talk about finding a job.
ML engineers need to deploy models and monitor them in production because without deploying models, you're not actually solving a problem. You of course need a PhD. You need at least four years of TensorFlow experience. Four years as a software engineer, you need to have publications and nirips or other top ML conferences.
Most undergrads in computer science these days graduate with ML experience. Another approach, instead of hiring for software engineering skills and training people on the ML side is to go more junior. And then the third way that you can do this more effectively is to be more specific about what you really, really need for not the ML engineering.
A lot of researchers, maybe through no fault of their own, focus on problems that are trendy. What you really want to look for, I think, is folks that have an independent sense of what problems are important to work on. This works a lot better if you do have experienced researchers who can provide mentorship and guidance for folks.
You can do a lot of this in person now that ML research conferences are back in person. Another thing is to build knowledge in an exciting field to like a more exciting branch of ML. Working on interesting data sets, this is kind of like one unique thing in ML. Being able to offer unique data sets can be pretty powerful.
One way to hire good ML people is to have other good MLpeople on the team. Emphasize how much they'll be able to learn about ML at your company. Sell the mission of the company and the potential for ML to have an impact on that mission.
For software engineers, you want to make sure that they at least meet a minimum bar on machine learning knowledge. Math puzzles are often common, especially involving things like linear algebra, take home projects. Another common assessment is probing the past projects that you've listed on your resume.
The first question I typically hear is like, where should I even look for ML jobs? Your standard sources like LinkedIn and recruiters all work. ML research conferences can also be a fantastic place. for many ML organizations hiring for software engineering is in many ways more important than hiring for ML skills.
The disadvantage if you're going to go in and work in an organization at this stage is that there's often little support available for ML projects. You probably won't have any infrastructure that you can rely on. And it can be difficult to hire and retain good talent. Plus leadership in the company may not really be bought into how useful ML could be.
The next evolution of ML organizations, oftentimes, is embedding machine learning directly into business and product teams. These ML teams will report up into the sort of engineering or tech organizations directly instead of being in their own sort of reporting arm. A lot of tech companies, when they start adopting ML, sort of pretty quickly get into trouble.
Machine learning is often seen as a smaller bet by an organization that is making a big bet on machine learning. The disadvantages of building ML this way are oftentimes it can be hard to hire and develop great ML people. It can also be difficult to get these ML folks access to the resources that they need to be really successful.
The last ML organization archetype, the end state, the right way your organization is to be an ML first organization. The big disadvantage of this model is that it leads to handoffs. And that can add friction to the process that you as an ML team need to run in order to actually get your models into production.
In the ML R&D archetype, typically you'll prioritize research over software engineering skills. The ML team won't really have any ownership over the data or oftentimes even the skill sets to build data pipelines themselves. And similarly, they won't be responsible for deploying maintenance of the models that they deploy.
Research teams do tend to work pretty closely with software engineering teams to get things done. In some cases, the ML team is actually the one that owns company wide data infrastructure. The number of participating teams was still growing really rapidly over the course of that time. So the upshot is it's really hard to tell in advance how easier or hard something is.
Production ML is still somewhere between research and engineering. There's cultural gaps that exist. These folks tend to come from different backgrounds. They have different values, goals, and norms. But you also have to manage up to help leadership understand your progress and what the outlook is for the thing you're building.
Open AI was doing project planning probabilistically. So rather than assuming that like a particular task is going to take a certain amount of time, instead we assign probabilities to let the likelihood of completion of each of these tasks and potentially pursue alternate tasks that allow us to unlock the same dependency in parallel.
For ML practitioners, it can be really natural to think about where ML can and can't be used. But for a lot of technologists or business leaders that are new to ML, the uses of ML that are practical can be kind of counterintuitive. Another common point of friction in dealing with the rest of the organization is valuing one side more than the other.
ML is inherently probabilistic. And that means that it will fail in production. A lot of times where ML efforts get hung up is in the same stakeholders, potentially that champion the project to begin with. The last thing that I'll say on educating the rest of the organization on ML is that MLPMs, I think play like one of the most critical roles in doing this effectively.
MLPMs are responsible for managing the workflow in and out of the ML team. They help filter out projects that aren't really high priority for the business or aren't good uses of ML. MLPMs also help educate the rest of the organization on what's possible to do with ML.
There's a couple of emerging ML project management methodologies. The first is Chris DM, which is actually an older methodology. The second is the team data science process, TDSP from of roles, responsibilities, templates. If you need an actual like granular project management framework, then I would start by trying TDSP.
The implication is that there's a big gap between users mental model for machine learning products and what they actually get for machinelearning products. Oftentimes they think that this product knows me better than I know myself because that's all the data about me from every interaction I've ever had with software.
One of the best practices for ML product design is explaining the benefits and limitations of the system to users. Focus on what problem the product is actually solving for the user, not on the fact that it's AI powered. And similarly, the more open ended and human feeling you make the product experience, like allowing users to enter any information that they want.
product design is building in feedback loops with your users. The X axis is how easy it is to use the feedback that you get in order to actually directly make your model better. The Yaxis is how much friction does it add to your users to collect this feedback.
To get more directly useful signals from your users, you can consider collecting direct implicit feedback. Explicit feedback is where you ask your user directly to provide feedback on the model's performance. The lowest friction way to do this for users tends to be to give them some sort of binary feedback mechanism, which can be like a thumbs up or thumbs down button.
Unstructured feedback about malls predictions can be very difficult to use as a model developer. One way to get granular feedback on malls predictions is to have like some sort of free text input where users can tell you what they thought about prediction. This often manifests itself in support tickets or support requests for your model.
If the goal of users providing the feedback is to make the model better, then one way you can encourage them to do that is by making it explicit how the feedback will make their user experience better. The more explicit you can be, the shorter the time interval is between when they give the feedback and when they act good.
This is a pretty young and under explored topic. Here's a bunch of resources that I would recommend checking out if you want to learn more about this. Many of the examples that we use in the previous slides are pulled from these resources. But paradoxically, as an outsider, it can be difficult to break into the field.
you