,triple_sentences,summaries_triple_sentences
0,"Hey, everybody, welcome back. This week we're going to talk about something a little bit different than we do most weeks. Most weeks we talk about specific technical aspects of building machine learning powered products, but this week we're going to focus on some of the organizational things that you need to do in order to work together on ML powered products as part of an interdisciplinary team. So the reality of building ML powered products is that building any product well is really difficult. You have to figure out how to hire great people. You need to be able to manage of the organization against those requirements. But machine learning adds even more additional complexity to this. Machine learning talent tends to be very scarce and expensive to attract. Machine learning teams are not just a single role, but today they tend to be pretty interdisciplinary, which makes managing them an even bigger challenge. Machine learning projects often have unclear timelines, and there's a high degree of uncertainty to those timelines. Machine learning itself is moving super fast,","This week we focus on some of the organizational things that you need to do in order to work together on ML powered products as part of an interdisciplinary team. Machine learning talent tends to be very scarce and expensive to attract. Machinelearning projects often have unclear timelines, and there's a high degree of uncertainty to those timelines."
1,"And so that makes it very difficult to help the rest of the stakeholders in your organization understand what they can really expect from the technology that you're building and what is realistic for us to achieve. So throughout the rest of this lecture, we're going to kind of touch on some of these themes and cover different aspects of this problem of working together to build ML product as an organization. So here are the pieces that we're going to cover. We're going to talk about different roles that are involved in building ML products. We're going to talk about some of the unique aspects involved in hiring. or ML platform or ML info teams, machine learning engineers, machine learning researchers, or ML scientists, data scientists. So there's a bunch of different roles here and one kind of obvious question is, what's the difference between all these different things? So let's break down the job function that each of these roles plays within the context of building the ML product. Starting with the ML product manager, their goal is to work with the ML team, the business team, the users, and any other stakeholders to prioritize projects and make sure that they're being executed","The goal is to help the rest of the stakeholders in your organization understand what they can really expect from the technology that you're building and what is realistic for us to achieve. We're going to talk about some of the unique aspects involved in hiring. or ML platform or ML info teams, machine learning engineers, machinelearning researchers, or ML scientists, data scientists."
2,"infrastructure, some shared tools that can be used across the ML teams in your company. And they're working with tools like AWS, like Kafka or other data infrastructure tools and potentially working with ML infrastructure vendors as well to sort of bring best and breed from traditional data and software tools and this new category of ML vendors that are providing like ML ops tools together to create this sort of best solution for the specific problems that your company is trying to solve. Then we have the ML engineer. ML engineer is kind of a catch all is the ML researcher. So this is a role that exists in some organizations that the responsibility stops after the model has been trained. And so oftentimes these models are either handed off to some other team to productionize or these folks are focused on building models that are not yet production critical or forward looking. Maybe they're prototyping some use cases that might be useful down the lines for the organization. And their work product is a trained model and oftentimes it's a report or a code repo that describes what this model does","ML engineer is kind of a catch all is the ML researcher. This is a role that exists in some organizations that the responsibility stops after the model has been trained. These folks are focused on building models that are not yet production critical or forward looking. They're working with tools like AWS, like Kafka or other data infrastructure tools and potentially working with ML infrastructure vendors."
3,"answering business questions using analytics. So in some organizations that data scientist is the same as an ML researcher or an ML engineer, another organization's data science is a distinct function that is responsible for answering business questions using data. The ML work is the responsibility of an ML team. So the next thing that we'll talk about is what are the different skills that you actually need to be successful in these roles? We're going to plot this on a two by two. On the x axis is the amount of skill that you need in machine learning, like how much ML do you really need to know? ML ops or ML platform engineers is they source them from MLEs at their organization. It's oftentimes like an ML engineer who used to just work on one model and then frustrated by the lack of tooling. So decided to move into more of a platform role. The ML engineer since this is someone who is required to understand the models deeply and also be able to productionize them. This tends to be a rare mix of ML skills and software engineering skills. So there's sort of two paths that I typically see for folks becoming ML engineers. Oftentimes these are software engineers who have",Data science is a distinct function that is responsible for answering business questions using data. The ML work is the responsibility of an ML team. ML ops or ML platform engineers is they source them from MLEs at their organization. This tends to be a rare mix of ML skills and software engineering skills.
4,"residency that are explicitly designed to train people without a PhD in this distinct skill of research. Since data science is kind of like a catch all term for a bunch of different roles in different organizations, it also admits a variety of different backgrounds. And oftentimes these are undergrads who want to a data science specific program or their science PhDs who are making the transition into industry. And then lastly, MLPMs, oftentimes these folks come from a traditional product management background. is the distinction between a task ML engineer and a platform ML engineer. This is a distinction that was coined by Shreya Shankar in blog posts that's linked below. And the distinction is that some ML engineers are really responsible for like one ML pipeline or maybe a handful of ML pipelines that they're assigned to. And so they're the ones that are day in and day out responsible for making sure that this model is healthy, making sure that it's being updated frequently and that any failures are sort of being accounted for. These folks are often like very overburied","Data science is kind of like a catch all term for a bunch of different roles in different organizations, it also admits a variety of different backgrounds. Some ML engineers are really responsible for like one ML pipeline or maybe a handful of ML pipelines. And then lastly, MLPMs, oftentimes these folks come from a traditional product management background."
5,"So we've talked a little bit about what are some of the different roles in the process of building ML Power products. Now let's talk about hiring. So how to think about hiring ML specialists. And if you are an ML specialist looking for a job, how to think about making yourself more attractive as a job candidate. So a few different things that we'll cover here. The first is the AI talent gap, which is sort of the reality of ML hiring these days. And we'll talk about how to source for ML engineers if you're hiring folks. We'll talk about interviewing and then last say we'll talk about finding a job. that it tends to be less of a blocker than it used to be. Because we've had four years of folks switching careers into ML and four years of software engineers emerging from undergrad with at least a couple of ML classes in many cases under their belts. So there's more and more people now that are capable of doing ML, but there's still a gap. And in particular, that gap tends to be in folks that understand more than just the underlying technology, but also have experience in seeing how it fails and how to make its accessible.","How to think about hiring ML specialists. If you are an ML specialist looking for a job, how to make yourself more attractive as a job candidate. How to source for ML engineers if you're hiring folks. We'll talk about interviewing and then last say we will talk about finding a job."
6,"that are building production ML systems because I think one sort of failure mode that I've seen relatively frequently, especially for ML platform teams is if you just bring on folks with pure software engineering background, a lot of times it's difficult for them to understand the user requirements well enough in order to engineer things that actually solve the user's problems. Users here being the task MLs who are, you know, the ones who are going to be using the infrastructure data that we'll focus for the rest of the section mostly on these two roles ML engineer and ML scientist. So there's a right and a wrong way to hire ML engineers. nothing. They need to deploy these models and monitor them in production because without deploying models, you're not actually solving a problem. So in order to fulfill all these duties, you need these requirements as this unicorn MLE role. You of course need a PhD. You need at least four years of TensorFlow experience. Four years as a software engineer, you need to have publications and nirips or other top ML conferences, experience building large scale distributed systems. And so when you add all this up, hopefully it's becoming clear why this is the wrong way to hire","ML engineers need to deploy models and monitor them in production because without deploying models, you're not actually solving a problem. You of course need a PhD. You need at least four years of TensorFlow experience. Four years as a software engineer, you need to have publications and nirips or other top ML conferences."
7,"Another approach, instead of hiring for software engineering skills and training people on the ML side is to go more junior. Most undergrads in computer science these days graduate with ML experience. And so these are folks that have traditional computer science training and some theoretical ML understanding. So they have sort of the seeds of being good at both ML and software engineering, but maybe not a lot of experience in either one. And then the third way that you can do this more effectively is to be more specific about what you really, really need for not the ML engineering on hiring people to have those skills, not these aspirational skills that you don't actually really need for your company. Next, let's talk about a couple of things I've done to be important for hiring ML researchers. The first is a lot of folks when they're hiring ML researchers, they look first at the number of publications they have in top conferences. I think it's really critical to focus entirely on the quality of publications, not the quantity. And this unfortunately requires a little bit of judgment about what high quality research looks like.","Most undergrads in computer science these days graduate with ML experience. Another approach, instead of hiring for software engineering skills and training people on the ML side is to go more junior. And then the third way that you can do this more effectively is to be more specific about what you really, really need for not the ML engineering."
8,"is looking for researchers who have an eye for working on problems that really matter. A lot of researchers, maybe through no fault of their own, because of the incentives in academia, focus on problems that are trendy. If everyone else is publishing about reinforcement learning, then they'll publish about reinforcement learning. If everyone else is publishing about generative models, then they'll make an incremental improvement to generative models to get them a publication. But what you really want to look for, I think, is folks that have an independent sense of what problems are important to work on. Because in the context of your company, This works a lot better if you do have experienced researchers who can provide mentorship and guidance for folks. I probably wouldn't hire like a first researcher that doesn't have ML experience. And then it's also worth remembering that especially these days you really don't need a PhD to do ML research. Many undergrads have a lot of experience doing ML research and graduates of some of these industrial fellowship programs like Google's or Facebook's or OpenAI's have learned the basics of how to do research regardless of whether they have a PhD.","A lot of researchers, maybe through no fault of their own, focus on problems that are trendy. What you really want to look for, I think, is folks that have an independent sense of what problems are important to work on. This works a lot better if you do have experienced researchers who can provide mentorship and guidance for folks."
9,"be doing most of the work and are generally more recruitable because they tend to be more junior in their careers. Beyond looking at papers, you can also do something similar for good reimplementation of papers that like, so if you are, you know, looking at some hot new paper and a week later, there's a reimplementation of that paper that has high quality code and hits the main results, then chances are, whoever wrote that implementation is probably pretty good. And so they could be worth recruiting. You can do a lot of this in person now that ML research conferences are back in person. of the art research. Another thing is to build knowledge in an exciting field to like a more exciting branch of ML or application of ML, working with excellent people, probably pretty consistent across many technical domains, but certainly true in ML. Working on interesting data sets, this is kind of like one unique thing in ML, since the work that you can do is constrained in many cases to the data sets that you have access to, being able to offer unique data sets can be pretty powerful. Probably again, true in general, but I've noticed for a lot of ML folks in particular, it's important for them to feel like they're doing work that really not","You can do a lot of this in person now that ML research conferences are back in person. Another thing is to build knowledge in an exciting field to like a more exciting branch of ML. Working on interesting data sets, this is kind of like one unique thing in ML. Being able to offer unique data sets can be pretty powerful."
10,"something that we did at OpenAI where we dedicate back then a day per week just to be focused on learning new things, but you can do it less frequently than that. Professional development budgets, conference budgets, things like this that you can emphasize. And this is probably especially valuable if you're strategy is to hire more junior folks or more software engineering oriented folks and train them up in machine learning. Emphasize how much they'll be able to learn about ML at your company. One sort of hack to being able to hire good ML people is to have other good ML people on the team. This is maybe easier said than done, but how interesting that is to work with, how much data you have, and how unique it is that you have it. And then lastly, just like any other type of recruiting, selling the mission of the company and the potential for ML to have an impact on that mission can be really effective. Next, let's talk about ML interviews. What I would recommend testing for if you are on the interviewer side of an ML interview is to try to hire for strengths and meet a minimum bar for everything else. And this can help you avoid falling into the trap of looking for unicorn MLEs. So some things that you can test are you want to validate your hypothesis",One way to hire good ML people is to have other good MLpeople on the team. Emphasize how much they'll be able to learn about ML at your company. Sell the mission of the company and the potential for ML to have an impact on that mission.
11,"like decent code, if not, you know, really high quality production ready code. Because in context of working with the team, other people are going to need to use their code. And it's not something that everyone learns how to do when they're in grad school for ML. For software engineers, you want to make sure that they at least meet a minimum bar on machine learning knowledge. And this is really testing for like, are they passionate about this field that they have put in the requisite effort to learn the basics of ML? That's a good indication that they're going to learn ML quickly on the job if you're hiring them mostly for their software engineering skills. So what do ML interviews actually do? and ML code since bugs tend to be where we spend most of our time in machine learning. Math puzzles are often common, especially involving things like linear algebra, take home projects, other types of assessments include applied ML questions. So typically this will have the flavor of, hey, here's a problem that we're trying to solve with ML. Let's talk through the sort of high level pieces of how we'd solve it, what type of algorithm we'd use, what type of system we'd need to build to support it. Another common assessment is probing the past projects that you've listed on your resume or listed as","For software engineers, you want to make sure that they at least meet a minimum bar on machine learning knowledge. Math puzzles are often common, especially involving things like linear algebra, take home projects. Another common assessment is probing the past projects that you've listed on your resume."
12,"is there's a great book from Chipwin, the introduction to machine learning interviews book, which is available for free online, which is especially useful. I think if you're preparing to interview for machine learning roles, speaking of which, what else should you be doing if your goal is to find new job and machine learning? The first question I typically hear is like, where should I even look for ML jobs? Your standard sources like LinkedIn and recruiters all work. ML research conferences can also be a fantastic place. Just go up and talk to the folks that are standing around the booths at those conferences. They tend to be looking for candidates. for many ML organizations hiring for software engineering is in many ways more important than hiring for ML skills. If you can show that you have a prod knowledge of ML, so writing blog posts that synthesize a particular research area or articulating a particular algorithm in a way that is new or creative or compelling can be a great way to stand out. But even better than that is demonstrating an ability to ship ML projects. And the best way to do this, I think, if you are not working in ML full time right now is through","The first question I typically hear is like, where should I even look for ML jobs? Your standard sources like LinkedIn and recruiters all work. ML research conferences can also be a fantastic place. for many ML organizations hiring for software engineering is in many ways more important than hiring for ML skills."
13,"products and how to think about hiring for those roles or being hired for those roles. The next thing that we're going to talk about is how machine learning teams fit into the context of the rest of the organization. Since we're still in the relatively early days of adopting this technology, there's no real consensus yet in terms of the best way to structure an ML team. But what we'll cover today is taxonomy of some of the best practices for different maturity levels of organizations and how they think about structuring their ML teams. And so we'll think about this as scaling a mountain from least mature a lot of low hanging fruit often for ML to come in and help solve. But the disadvantage if you're going to go in and work in an organization at this stage is that there's often little support available for ML projects. You probably won't have any infrastructure that you can rely on. And it can be difficult to hire and retain good talent. Plus leadership in the company may not really be bought into how useful ML could be. So that's some of the things to think about if you're going to go take a role in one of these organizations. Once the company has decided, hey, this ML thing is something exciting, something that we should invest in,",The disadvantage if you're going to go in and work in an organization at this stage is that there's often little support available for ML projects. You probably won't have any infrastructure that you can rely on. And it can be difficult to hire and retain good talent. Plus leadership in the company may not really be bought into how useful ML could be.
14,"If you're going to go work in one of these organizations, one of the big advantages is you can get away with being less experienced on the research side. And since the ML team isn't really going to be on the hook today for any sort of meaningful business outcomes, another big advantage is that these teams can work on long term business priorities and they can focus on trying to get to what would be really big wins for the organization. But the disadvantageous to be aware of is if you're thinking about joining a team at this stage or building a team at this stage is that oftentimes since the ML team is sort of and they can't really justify doubling down. The next evolution of ML organizations, oftentimes, is embedding machine learning directly into business and product teams. So what this looks like is you'll have some product teams within the organization that have a handful of ML people side by side with their software or analytics teams. And these ML teams will report up into the sort of engineering or tech organizations directly instead of being in their own sort of reporting arm. A lot of tech companies, when they start adopting ML, sort of pretty quickly get","The next evolution of ML organizations, oftentimes, is embedding machine learning directly into business and product teams. These ML teams will report up into the sort of engineering or tech organizations directly instead of being in their own sort of reporting arm. A lot of tech companies, when they start adopting ML, sort of pretty quickly get into trouble."
15,"cycle between new ideas that they have for how to make the ML better, how to make the product better with ML into actual results as part of the products. The disadvantages of building ML this way are oftentimes it can be hard to hire and develop really, really great ML people because great ML people often want to work with other great ML people. It can also be difficult to get these ML folks access to the resources that they need to be really successful. So that's the infrastructure they need, the data they need, or the compute they need because they don't have sort of a central team What this looks like is you'll have a machine learning division of the company that reports up to senior leadership So they report to the CEO or the CTO or something along those lines This is what distinguishes it from the MLR and D archetype where the ML team is often you know reporting to someone more junior in the organization often a foreigner as sort of a smaller bet This is the organization making a big bet to investing in machine learning So oftentimes this is also the archetype where you'll start to see MLPMs or platform MLPMs that work with researchers and ML engineers",Machine learning is often seen as a smaller bet by an organization that is making a big bet on machine learning. The disadvantages of building ML this way are oftentimes it can be hard to hire and develop great ML people. It can also be difficult to get these ML folks access to the resources that they need to be really successful.
16,"you can in other archetypes. And it makes it a lot easier when you have a central as organization to invest in things like tooling and infrastructure and culture and best practices around developing ML in your organization. The big disadvantage of this model is that it leads to handoffs. And that can add friction to the process that you as an ML team need to run in order to actually get your models into production. And the last ML organization archetype, the end state, the goal if you're trying to build ML, the right way your organization is to be an ML first organization. So what this looks like is as well as startups that were founded with ML as a core guiding principle for how they want to build the products. And these days more and more, you're starting to see other tech companies who began investing in ML four or five years ago to start to become closer to this archetype. There's mostly advantages to this model. You have great access to data. It's relatively easy to recruit. And most importantly, it's probably easiest in this archetype out of all of them to get value out of ML because the products teams that you're working with understand machine learning. And really, the only disadvantage of this model","The last ML organization archetype, the end state, the right way your organization is to be an ML first organization. The big disadvantage of this model is that it leads to handoffs. And that can add friction to the process that you as an ML team need to run in order to actually get your models into production."
17,"The second question is data ownership. So is the ML team also responsible for creating publishing data? Or do they just consume that from other teams? And the last thing is model ownership. The ML team, are they the ones that are going to productionize models or is that the responsibility of some other team? In the ML R&D archetype, typically you'll prioritize research over software engineering skills. And the ML team won't really have any ownership over the data or oftentimes even the skill sets to build data pipelines themselves. And similarly, they won't be responsible for deploying maintenance of the models that they deploy in the ML function archetype. Typically, the requirement will be that you'll need to have a team that has a strong mix of software engineering research and data skills. So the team size here starts to become larger. A minimum might be something like one data engineer, one ML engineer, potentially a platform engineer or DevOps engineer and potentially a PM. But these teams are often working with a bunch of other functions so they can in many cases get much larger than that. And you know, in many cases in the organizations, you'll have both software engineers and researchers working","In the ML R&D archetype, typically you'll prioritize research over software engineering skills. The ML team won't really have any ownership over the data or oftentimes even the skill sets to build data pipelines themselves. And similarly, they won't be responsible for deploying maintenance of the models that they deploy."
18,"but research teams do tend to work pretty closely with software engineering teams to get things done. In some cases, the ML team is actually the one that owns company wide data infrastructure because ML is such a central bet for the company that it makes sense for the ML team to make some of the sort of main decisions about how data will be organized. And then finally, if the ML team is the one that actually built the model, they'll typically hand it off to a user who, since they have the basic ML skills and knowledge to do this, they'll actually be the one to maintain the model. And here's all of this on one slide. performing model. They went from 35 to 70% accuracy within one week. And they were thinking, this is great. Like we're going to hit 95% accuracy. And this contest is going to be huge success. But then if you zoom out and look at the entire course of the project over three months, it turns out that most of that accuracy gain came in the first week and the impermanence thereafter were just marginal. And that's not because of a lack of effort. The number of participating teams was still growing really rapidly over the course of that time. So the upshot is it's really hard to tell in advance how easier or hard something is","Research teams do tend to work pretty closely with software engineering teams to get things done. In some cases, the ML team is actually the one that owns company wide data infrastructure. The number of participating teams was still growing really rapidly over the course of that time. So the upshot is it's really hard to tell in advance how easier or hard something is."
19,"clear what approach will actually work for training a model that's good enough to solve the problem. And the upshot of all of this is that estimating the timeline for a project when you're in the project planning phase can be very difficult. In other words, production ML is still somewhere between research and engineering. Another challenge for managing ML teams is that there's cultural gaps that exist between research and engineering organizations. These folks tend to come from different backgrounds. They have different training. They have different values, goals, and norms. For example, oftentimes, you know, stereotypically But you also have to manage up to help leadership understand your progress and what the outlook is for the thing that you're building since ML is such a new technology many leaders and organizations even in good technology organizations don't really understand it So next I want to talk about some of the ways that you can manage machine learning projects better and the first approach that I'll talk about is doing project planning probablyistically So oftentimes when we think about project planning for software projects, we think about it as sort of a waterfall right where you have a","Production ML is still somewhere between research and engineering. There's cultural gaps that exist. These folks tend to come from different backgrounds. They have different values, goals, and norms. But you also have to manage up to help leadership understand your progress and what the outlook is for the thing you're building."
20,"it open AI was doing project planning probabilistically. So rather than assuming that like a particular task is going to take a certain amount of time, instead we assign probabilities to let the likelihood of completion of each of these tasks and potentially pursue alternate tasks that allow us to unlock the same dependency in parallel. So in this example, you know, maybe task V and task C are both alternative approaches to unlocking task D. So we might do both them at the same time. And so if we realize all the sudden that task C is not going to work and task doesn't necessarily mean that you need to do them all in parallel, but many good machine learning organizations do. So one way to think about this is, you know, if you know that you need to build like a model that's never been built in your organization before, you can have like a friendly competition of ideas. If you have a culture that's built around working together as a team to get to the right answer and not just rewarding the one person who solves the problem correctly. Another corollary to this idea that that many machine learning ideas can and will fail is that when you're doing performance management, it's important not to get hung up on just","Open AI was doing project planning probabilistically. So rather than assuming that like a particular task is going to take a certain amount of time, instead we assign probabilities to let the likelihood of completion of each of these tasks and potentially pursue alternate tasks that allow us to unlock the same dependency in parallel."
21,"valuing one side more than the other. So thinking engineering is more important than research, which can lead to things getting stuck on the ML side because the ML side is not getting the resources or attention that they deserve or thinking that research is more important than engineering, which can lead to creating ML innovations that are not actually useful. So oftentimes the way around this is to have engineers and researchers work very closely together. In fact, like sometimes uncomfortably close together, like working together on the same code base for the same project and understanding that these folks bring different skill sets a lot of people in the organization don't understand one or more of these things. For us as ML practitioners, it can be really natural to think about where ML can and can't be used. But for a lot of technologists or business leaders that are new to ML, the uses of ML that are practical can be kind of counterintuitive. And so they might have ideas for ML projects that are feasible and they might miss ideas for ML projects that are pretty easy that don't fit their mental model of what ML can use. Another common point of friction in dealing with the rest of the organization is","For ML practitioners, it can be really natural to think about where ML can and can't be used. But for a lot of technologists or business leaders that are new to ML, the uses of ML that are practical can be kind of counterintuitive. Another common point of friction in dealing with the rest of the organization is valuing one side more than the other."
22,"heads around the fact that ML is inherently probabilistic. And that means that it will fail in production. And so a lot of times where ML efforts get hung up is in the same stakeholders, potentially that champion the project to begin with, not really being able to get comfortable with the fact that once the model is out in the world, it's, you know, the users are going to start to see failures that it makes in almost all cases. And the last common failure mode in working with the rest of the organization is the rest of the organization treating ML projects like other software projects and not realizing that they need to be I would recommend Peter Beale's AI strategy class from the business school at UC Berkeley and Google's People in AI guidebook, which will be referring to a lot more in the rest of the lecture as well. The last thing that I'll say on educating the rest of the organization on ML is that MLPMs, I think play like one of the most critical roles in doing this effectively. To illustrate this, I'm going to make an analogy to the two types of ML engineers and describe two prototypical types of MLPMs that I see in different organizations.","ML is inherently probabilistic. And that means that it will fail in production. A lot of times where ML efforts get hung up is in the same stakeholders, potentially that champion the project to begin with. The last thing that I'll say on educating the rest of the organization on ML is that MLPMs, I think play like one of the most critical roles in doing this effectively."
23,"MLPMs tend to start to make sense when you have a centralized ML team and that centralized ML team needs to play some role in educating the rest of the organization in terms of what are productive uses of ML in all the products that the organization is building. These folks are responsible for managing the workflow in and out of the ML team. So helping filter out projects that aren't really high priority for the business or aren't good uses of ML, helping proactively find projects that might have a big impact on the product or the company by spending from the other product functions and gathering requirements from them, but also helping educate them on what's possible to do with ML and helping them come up with ideas to use ML in their areas of responsibility that they find exciting so that they can over time really start to build their own intuition about what types of things they should be considering ML to be used for. And then another really critical role that these platform ML PMs can play is mitigating the risks of, you know, we've built a model, but can't convince the rest of the organization to actually use it by being really crisp about what are the requirements",MLPMs are responsible for managing the workflow in and out of the ML team. They help filter out projects that aren't really high priority for the business or aren't good uses of ML. MLPMs also help educate the rest of the organization on what's possible to do with ML.
24,"is what's the equivalent of Agile or any of these established development methodologies for software in ML? Is there something like that that we can just take off the shelf and apply and deliver successful ML products? And the answer is there's a couple of emerging ML project management methodologies. The first is Chris DM, which is actually an older methodology, but it was originally focused on data mining and has been subsequently applied to data science in ML. And the second is the team data science process, TDSP from of roles, responsibilities, templates that you can use to actually execute on this process. Chris DM is a bit higher level. So if you need an actual like granular project management framework, then I would start by trying TDSP. But I'll say more generally, it's reasonable to use these if you truly have a large scale coordination problem. If you're trying to get a large ML team working together successfully for the first time, but I would otherwise recommend skipping these because they're more focused on traditional data mining or data science processes and they'll probably slow you down. So I would sort of","There's a couple of emerging ML project management methodologies. The first is Chris DM, which is actually an older methodology. The second is the team data science process, TDSP from of roles, responsibilities, templates. If you need an actual like granular project management framework, then I would start by trying TDSP."
25,"of the world that it has achieved by reading the whole internet. Oftentimes they think that this product knows me better than I know myself because that's all the data about me from every interaction I've ever had with software. They think that AI powered products learn from their mistakes and that they generalize to new problems, right? Because it's intelligence. It's able to learn from new examples to solve new tasks. But I think a better mental model for where you actually get with an ML powered products is a dog that you train to solve a puzzle, right? So it's amazing that it can to adapt general knowledge to new tasks or new contexts. Dogs are great at learning tricks, but they can't do it if you don't give them treats. And similarly, machine learning systems don't tend to learn well without feedback or rewards in place to help understand where they're performing well and where they're not performing well. And lastly, both dogs learning tricks and machine learning systems might misbehave if you leave them unattended. The implication is that there's a big gap between users mental model for machine learning products and what they actually get for machine learning products.",The implication is that there's a big gap between users mental model for machine learning products and what they actually get for machinelearning products. Oftentimes they think that this product knows me better than I know myself because that's all the data about me from every interaction I've ever had with software.
26,"users to actually improve the system. One of the best practices for ML product design is explaining the benefits and limitations of the system to users. One way that you can do that is since users tend to have misconceptions about what AI can and can't do, focus on what problem the product is actually solving for the user, not on the fact that it's AI powered. And similarly, the more open ended and human feeling you make the product experience, like allowing users to enter any information that they want to or ask likely to be able to understand. And then finally, the reality is that your model has limitations. And so you should explain those limitations to users and consider actually just baking those limitations into the model as guardrails. So not letting your users provide inputs to your model that you know the model is not going to perform well on. So that could be as simple as, you know, if your NLP system was designed to perform well on English text, then detecting if users input text in some other language and, you know, either warning them or not allowing them to input text in a language where your model is not","One of the best practices for ML product design is explaining the benefits and limitations of the system to users. Focus on what problem the product is actually solving for the user, not on the fact that it's AI powered. And similarly, the more open ended and human feeling you make the product experience, like allowing users to enter any information that they want."
27,"and auto tagging feature of recognizing your face and pictures and suggesting who the person was, they didn't just assign the tag to the face, even though they almost always knew exactly who that person was, because it'd be a really bad experience if all of a sudden you were tagged in some picture of someone else. Instead, they just add like simple yes, no, that lets you confirm that they in fact got the prediction that this is your face correctly. In order to mitigate the effect of when the model inevitably does make some bad predictions, there's a couple of patterns that can help there. The first is, product design is building in feedback loops with your users. So let's talk about some of the different types of feedback that you might collect from your users. And the X axis is how easy it is to use the feedback that you get in order to actually directly make your model better. And the Y axis is how much friction does it add to your users to collect this feedback. So roughly speaking, you could think about like above this line on the middle of the chart is implicit feedback that you collect from your users without needing to change their behavior. And on the right side of the chart are signals that you can train on directly.",product design is building in feedback loops with your users. The X axis is how easy it is to use the feedback that you get in order to actually directly make your model better. The Yaxis is how much friction does it add to your users to collect this feedback.
28,"because these are high level sort of business outcomes that may depend on many other things other than just your model's prediction. So to get more directly useful signals from your users, you can consider collecting direct implicit feedback where you collect signals from the products that measure how useful this prediction is to the user directly rather than indirectly. For example, if you're given the user recommendation, you can measure whether they clicked on the recommendation or if you're suggesting an email for them to send, did they send that email or did they copy the suggestion so that they can use it in some other application? will move on to explicit types of user feedback. Explicit feedback is where you ask your user directly to provide feedback on the model's performance. And the lowest friction way to do this for users tends to be to give them some sort of binary feedback mechanism, which can be like a thumbs up or thumbs down button in your product. This is pretty easy for users because it just requires them to like click one button and it could be a decent training signal. There's some research and using signals like this in order to guide the learning process of models to be more aligned with users preferences.","To get more directly useful signals from your users, you can consider collecting direct implicit feedback. Explicit feedback is where you ask your user directly to provide feedback on the model's performance. The lowest friction way to do this for users tends to be to give them some sort of binary feedback mechanism, which can be like a thumbs up or thumbs down button."
29,"way you can get more granular feedback on malls predictions is to have like some sort of free text input where users can tell you what they thought about prediction. This often manifests itself in support tickets or support requests for your model. This requires a lot of work on the part of your users and it can be very difficult to use as a model developer because you have to parse through this like unstructured feedback about your malls predictions. Yet it tends to be quite useful sometimes in practice because since it's high friction to actually provide this kind of feedback, the feedback that users do provide can be very high within the same product experience that you're building. Not is this useful for them to copy and use in a different app, but is it useful for them to use within my app? So one example of this is in a product called GreatScope, which Sergey built. There is a model that when students submit their exams, it tries to match the handwritten name on the exam with the name of the student in the student registry. Now, if the model doesn't really know who that student is,",Unstructured feedback about malls predictions can be very difficult to use as a model developer. One way to get granular feedback on malls predictions is to have like some sort of free text input where users can tell you what they thought about prediction. This often manifests itself in support tickets or support requests for your model.
30,"for users to actually spend the time to give us feedback on this. The sort of most foolproof way of doing this is as we described before, to gather feedback as part of an existing user workflow. But if that's not possible, if the goal of users providing the feedback is to make the model better, then one way you can encourage them to do that is to make it explicit how the feedback will make their user experience better. And generally speaking, like the more explicit you can be here and the shorter the time interval is between when they give the feedback and when they act good is the example to the right of that where the response to the feedback just says, thank you for your feedback. Because as a user, when I give that feedback, there's no way for me to know whether that feedback is actually making the product experience better. So it discourages me from getting more feedback in the future. The main takeaway on product design for machine learning is that great ML powered products and product experiences are not just, you know, taking existing product that works well in both ML and top of it. They're actually designed from scratch with machine learning and the particularly","If the goal of users providing the feedback is to make the model better, then one way you can encourage them to do that is by making it explicit how the feedback will make their user experience better. The more explicit you can be, the shorter the time interval is between when they give the feedback and when they act good."
31,"is the perfect version of the model doesn't exist and certainly doesn't exist in the first version of the model that you deploy. And so one important thing to think about when you're designing your product is how can you help your users make the product experience better by collecting the right type of feedback from them. This is a pretty young and under explored topic. And so here's a bunch of resources that I would recommend checking out if you want to learn more about this. Many of the examples that we use in the previous slides are pulled from these resources. And in particular, the resource from Google in the top bullet point is really good if you want to understand the basics. need for these roles. But paradoxically, as an outsider, it can be difficult to break into the field. And the sort of main recommendation that we had for how to get around that is by using projects to build awareness of your thinking about machine learning. The next thing that we talk about is how machine learning teams fit into the broader organization. We covered a bunch of different archetypes for how that can work. And we looked at how machine learning teams are becoming more standalone and more interdisciplinary and how they function. Next, we talked about managing ML teams and managing ML products. Managing ML teams is hard. And there's no silver bullet here.","This is a pretty young and under explored topic. Here's a bunch of resources that I would recommend checking out if you want to learn more about this. Many of the examples that we use in the previous slides are pulled from these resources. But paradoxically, as an outsider, it can be difficult to break into the field."
32,you,you
